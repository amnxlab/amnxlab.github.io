<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Yuhui Zhang - Stanford CS PhD">
    <meta name="keywords" content="Yuhui Zhang, Stanford, CS PhD, Computer Science, AI, Machine Learning, Multi-modal ML, Vision-Language Models">
    <meta name="author" content="Yuhui Zhang">
    <meta property="og:title" content="Yuhui Zhang - Stanford CS PhD">
    <meta property="og:description" content="Research in multi-modal machine learning, foundation models, and applications to biomedicine and broad science.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://cs.stanford.edu/~yuhuiz/">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:creator" content="@Zhang_Yu_hui">
    <title>Yuhui Zhang's Homepage</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;500;600&display=swap">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="styles.css?v=20251026">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-RYWTGY2KVN"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-RYWTGY2KVN');
    </script>
</head>
<body id="top">
    <header>
        <div class="header-content">
            <h1>Yuhui Zhang</h1>
            <p>Multimodal Machine Learning & Science Applications | Stanford University</p>
            <div class="social-links-header">
                <a href="https://scholar.google.com/citations?user=X-Agfu8AAAAJ"><i class="fas fa-graduation-cap"></i>Google Scholar</a>
                <a href="https://twitter.com/Zhang_Yu_hui"><i class="fab fa-twitter"></i>Twitter</a>
                <a href="https://www.linkedin.com/in/yuhui-zhang-223325121/"><i class="fab fa-linkedin"></i>LinkedIn</a>
                <a href="https://github.com/yuhui-zh15"><i class="fab fa-github"></i>GitHub</a>
                <a href="mailto:yuhuiz@stanford.edu"><i class="fas fa-envelope"></i>Email</a>
            </div>
            <!-- <nav>
                <ul>
                    <li><a href="#about">About</a></li>
                    <li><a href="#research">Research</a></li>
                    <li><a href="#news">News</a></li>
                    <li><a href="#publications">Publications</a></li>
                    <li><a href="#awards">Awards</a></li>
                    <li><a href="#services">Services</a></li>
                    <li><a href="#miscellaneous">Miscellaneous</a></li>
                </ul>
            </nav> -->
        </div>
        <div class="header-image">
            <img src="images/yuhui.jpeg" alt="Yuhui Zhang Profile Photo">
        </div>
    </header>

    <main>
        <section id="about">
            <p>Hi! I am a postdoctoral scholar at Stanford University, working with Professors <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>, <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>, and <a href="https://lundberglab.stanford.edu/">Emma Lundberg</a>. Previously, I received my Ph.D. in Computer Science from Stanford University and my B.E. in Computer Science from Tsinghua University.</p>

            <p><strong>I am on the job market this year (2025-2026).</strong></p>
        </section>

        <section id="research">
            <h2>Research</h2>
            <p>My research focuses on the <strong>foundations of multimodal machine learning</strong> (e.g., vision and language) and its <strong>applications to science</strong> (e.g., computational cell biology)</strong>. My recent works explore:</p>
            <ul>
                <li>Embedding-based multimodal foundation models (<a href="https://arxiv.org/pdf/2401.08567.pdf">ICLR'24</a>, <a href="https://arxiv.org/pdf/2302.04269.pdf">ICLR'23</a>, <a href="https://arxiv.org/pdf/2203.02053.pdf">NeurIPS'22</a>)</li>
                <li>Generative multimodal foundation models (<a href="https://arxiv.org/pdf/2405.18415.pdf">NeurIPS'24</a>, <a href="https://arxiv.org/pdf/2311.16201">EMNLP'24</a>, <a href="https://arxiv.org/pdf/2305.17311.pdf">ACL'23</a>)</li>
                <li>Agentic/compounding systems (<a href="https://arxiv.org/pdf/2501.03225.pdf">CVPR'25</a>, <a href="https://arxiv.org/pdf/2312.02974.pdf">CVPR'24</a>, <a href="https://arxiv.org/pdf/2403.10517.pdf">ECCV'24</a>)</li>
                <li>Generative models for science (<a href="https://www.arxiv.org/abs/2502.09775">ICML'25</a>, <a href="https://arxiv.org/pdf/2310.01783.pdf">NEJM AI'24</a>, <a href="https://www.nature.com/articles/s41746-019-0113-1.pdf">npj Digital Medicine'19</a>)</li>
            </ul>
        </section>

        <section id="news">
            <h2>News</h2>
            <div class="news-container">
                <ul class="news-list">
                    <li>10/2025: Received my Ph.D. degree from Stanford University and started as a postdoctoral scholar. Huge thanks to my advisors and committee members: Professors <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>, <a href=https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>, <a href="https://thashim.github.io/">Tatsunori Hashimoto</a>, <a href="https://lundberglab.stanford.edu/">Emma Lundberg</a>, and <a href="https://cioffi-group.stanford.edu/">John Cioffi</a>!</li>
                    <li>09/2025: Honored to be selected as one of the <a href="https://datascience.stanford.edu/programs/rising-stars-data-science">Rising Stars in Data Science</a>.</li>
                    <li>08/2025: Our study about <a href="https://www.arxiv.org/pdf/2510.11835">CLIP vs DINO</a> & <a href="https://arxiv.org/pdf/2502.12466">Code Equivalence Checking</a> are accepted to EMNLP 2025, and <a href="https://arxiv.org/pdf/2509.25851">Multimodal Symbolic Reasoning</a> is accepted to NeurIPS 2025.</li>
                    <li>07/2025: Presented <a href="https://www.arxiv.org/abs/2502.09775">CellFlux</a> at ICML 2025. Thanks <a href="https://www.citadel.com/careers/programs-and-events/conference-travel-grant/">Citadel</a> for providing the travel grant!</li>
                    <li>06/2025: Honored to be selected as one of the participants in <a href="https://cvpr.thecvf.com/Conferences/2025/CallForDoctoralConsortium">CVPR 2025 Doctoral Consortium</a>. Also excited to host <a href="https://dcvlr-neurips.github.io/">DCVLR: Data Curation for Vision Language Reasoning</a> challenge at NeurIPS 2025.</li>
                    <li>05/2025: Our latest advance in virtual cell modeling (a.k.a. world model for cells), <a href="https://www.arxiv.org/abs/2502.09775">CellFlux (formerly CellFlow)</a>, has been accepted to ICML 2025. <a href="https://arxiv.org/pdf/2505.22946">NegVQA</a> has been accepted to ACL 2025 Findings.</li>
                    <li>04/2025: Three papers presented at ICLR 2025: <a href="https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/">VLM Interpretability</a>, <a href="https://openreview.net/pdf?id=3bcN6xlO6f">VidDiff</a>, <a href="https://arxiv.org/pdf/2306.09479">Inverse Scaling</a>.</li>
                    <li>03/2025: Three papers accepted to CVPR 2025: <a href="https://arxiv.org/pdf/2501.03225">AutoConverter</a>, <a href="https://arxiv.org/pdf/2503.13399">MicroVQA</a>, <a href="https://arxiv.org/pdf/2501.07171">BIOMEDICA</a>. We are organizing <a href="https://dataworldicml2025.github.io/">DataWorld Workshop</a> at ICML 2025.</li>
                    <li>02/2025: Introduce <a href="https://www.arxiv.org/abs/2502.09775">CellFlow</a>, a flow-matching based method for cellular morphology prediction. We are organizing <a href="https://xai4cv.github.io/workshop_cvpr25">XAI4CV Workshop</a> at CVPR 2025, <a href="https://sites.google.com/view/mmfm-biomed-2025/">MMFM-BIOMED Workshop</a> at CVPR 2025, <a href="https://xllms.github.io/">XLLM Workshop</a> at ACL 2025.</li>
                    <li>01/2025: Introduce <a href="https://yuhui-zh15.github.io/AutoConverter-Website/">AutoConverter</a>, an agentic framework to convert open-ended VQA questions into the multiple-choice format. <a href="https://openreview.net/pdf?id=3bcN6xlO6f">VidDiff</a> accepted to ICLR 2025, <a href="https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/">VLM Interpretability</a> accepted to ICLR 2025 Blog Track.</li>
                    <li>12/2024: Two papers presented at NeurIPS 2024 and two at ML4H 2024.</li>
                    <li>11/2024: Our <a href="https://arxiv.org/pdf/2311.16201">work</a> is selected as an oral presentation (198/6105) at EMNLP 2024! Also selected as one of NeurIPS 2024 Top Reviewers and EMNLP 2024 Outstanding Reviewers.</li>
                    <li>10/2024: <a href="https://yuhui-zh15.github.io/VLMClassifier-Website/">VLMClassifier</a> is accepted to NeurIPS 2024; <a href="https://arxiv.org/pdf/2407.01791">Micro-Bench</a> is accepted to NeurIPS 2024 Datasets and Benchmarks Track.</li>
                    <li>09/2024: Our <a href="https://arxiv.org/pdf/2311.16201">work</a> analyzing pre-trained language models for image generation is accepted to EMNLP 2024 main conference.</li>
                    <li>07/2024: <a href="https://wxh1996.github.io/VideoAgent-Website/">VideoAgent</a> is accepted to ECCV 2024; <a href="https://ai.nejm.org/doi/full/10.1056/AIoa2400196">AI scientific feedback</a> is published in NEJM AI.</li>
                    <li>06/2024: Our new <a href="https://arxiv.org/pdf/2405.18415.pdf">work</a> investigates why visually-grounded language models are bad at the basic image classification task.</li>
                    <li>05/2024: Selected as a Citadel GQS Fellowship finalist and gave a talk in Chicago.</li>
                    <li>04/2024: <a href="https://understanding-visual-datasets.github.io/VisDiff-website/">VisDiff</a> is selected as an oral presentation (90/11532) at CVPR 2024!</li>
                    <li>03/2024: Introduce <a href="https://wxh1996.github.io/VideoAgent-Website/">VideoAgent</a>, where we leverage a large language model as an agent for long-form video understanding.</li>
                    <li>02/2024: <a href="https://understanding-visual-datasets.github.io/VisDiff-website/">VisDiff</a> accepted to CVPR 2024.</li>
                    <li>01/2024: ICLR 2024: <a href="https://yuhui-zh15.github.io/C3-Website/">C3</a> explains the geometry in multi-modal contrastive representation space and introduces a three-step method to bridge the modality gap.</li>
                    <li>12/2023: Introduce <a href="https://understanding-visual-datasets.github.io/VisDiff-website/">VisDiff</a>, an algorithm that automatically describes differences between two image sets, joint work with Berkeley AI Research!</li>
                    <li>11/2023: Honored to be selected as one of NeurIPS 2023 Top Reviewers.</li>
                    <li>10/2023: Large language models <a href="https://arxiv.org/pdf/2310.01783.pdf">generate scientific feedback</a>, <a href="https://arxiv.org/pdf/2310.19677.pdf">answer moral and causal questions</a>, <a href="https://openreview.net/pdf?id=DwgRm72GQF">show inverse scaling on 11 tasks</a>.</li>
                    <li>05/2023: Larger language models are not necessarily better on all the tasks. Check <a href="https://arxiv.org/pdf/2305.17311v1.pdf">our work</a> in ACL 2023 Findings!</li>
                    <li>01/2023: Can you diagnose and rectify a vision model using language inputs? Check <a href="https://openreview.net/pdf?id=D-zfUK7BR6c">our work</a> in ICLR 2023!</li>
                    <li>11/2022: We won the 3rd prize in the first-round <a href="https://twitter.com/EthanJPerez/status/1574488551839789056">Inverse Scaling Prize</a>! Also check out <a href="https://arxiv.org/pdf/2211.09110.pdf">HELM</a> that holistically evaluates language models.</li>
                    <li>10/2022: Honored to receive a NeurIPS 2022 Scholar Award. Thank you NeurIPS organizers!</li>
                    <li>10/2022: Two more works will be presented in ML4H and NeurIPS 2022!</li>
                    <li>09/2022: Our work studying <a href="https://openreview.net/pdf?id=S7Evzt9uit3">the modality gap</a> accepted to NeurIPS 2022</a>!</li>
                    <li>07/2020: Stanza now supports <a href="http://stanza.run/bio">biomedical and clinical text processing</a>!</li>
                    <li>03/2020: Announce <a href="https://github.com/stanfordnlp/stanza">Stanza</a>: A Python NLP Library for Many Human Languages! <a class="github-button" href="https://github.com/stanfordnlp/stanza" data-icon="octicon-star" data-show-count="true" aria-label="Star stanfordnlp/stanza on GitHub">Star</a></li>
                    <li>05/2019: Selected as the best oral presentation at 36th Tsinghua CS Forum for Graduate Students!</li>
                    <li>04/2019: How to infer thousands of diagnoses from EHRs? Check our <a href="https://www.nature.com/articles/s41746-019-0113-1.pdf">paper</a> in npj (Nature) Digital Medicine!</li>
                    <li>12/2018: Awarded the SenseTime Scholarship (USD 3,000). Thanks <a href="https://www.sensetime.com/en/">SenseTime Inc.</a>!</li>
                    <li>10/2018: Awarded highly selective National Scholarship!</li>
                    <li>06/2018: Received Tsinghua Research Fellowship with a funding of 7,500 USD!</li>
                </ul>
            </div>
        </section>

        <section id="publications">
            <div class="publications-header">
                <h2 id="toggle-header">Selected Publications</h2>
                <button id="toggle-publications" class="toggle-publications">Show All</button>
            </div>
            <div id="publications-container">
                <!-- Publications will be dynamically loaded here -->
            </div>
        </section>

        <section id="awards">
            <h2>Awards</h2>
            <div class="award-item">
                <span class="award-year">2025</span>
                <div class="award-desc">Rising Stars in Data Science</div>
            </div>
            <div class="award-item">
                <span class="award-year">2025</span>
                <div class="award-desc">CVPR Doctoral Consortium</div>
            </div>
            <div class="award-item">
                <span class="award-year">2025</span>
                <div class="award-desc">Citadel ICML Travel Grant</div>
            </div>
            <div class="award-item">
                <span class="award-year">2024</span>
                <div class="award-desc">NeurIPS Top Reviewer</div>
            </div>
            <div class="award-item">
                <span class="award-year">2024</span>
                <div class="award-desc">EMNLP Outstanding Reviewer</div>
            </div>
            <div class="award-item">
                <span class="award-year">2024</span>
                <div class="award-desc">Citadel GQS Fellowship Finalist</div>
            </div>
            <div class="award-item">
                <span class="award-year">2023</span>
                <div class="award-desc">NeurIPS Top Reviewer</div>
            </div>
            <div class="award-item">
                <span class="award-year">2023</span>
                <div class="award-desc">Stanford Data Science Scholar Finalist</div>
            </div>
            <div class="award-item">
                <span class="award-year">2022</span>
                <div class="award-desc">NeurIPS Scholar Award</div>
            </div>
            <div class="award-item">
                <span class="award-year">2019</span>
                <div class="award-desc">Best Oral Presentation Award <small>(Presented VetTag at 36th Tsinghua CS Graduate Forum)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2019</span>
                <div class="award-desc">SenseTime Scholarship <small>(30 in China)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2018</span>
                <div class="award-desc">National Scholarship <small>(0.2% in China)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2018</span>
                <div class="award-desc">Qualcomm Scholarship <small>(100 in China)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2018</span>
                <div class="award-desc">Tsinghua Research Fellowship <small>(50 at Tsinghua)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2016-18</span>
                <div class="award-desc">Tsinghua Academic / Comprehensive Excellence Scholarship</div>
            </div>
            <div class="award-item">
                <span class="award-year">2015</span>
                <div class="award-desc">Freshman Scholarship <small>(300 at Tsinghua)</small></div>
            </div>
            <div class="award-item">
                <span class="award-year">2014</span>
                <div class="award-desc">Chinese Chemistry Olympiad (CChO) 1st Prize <small>(50 in China)</small></div>
            </div>
        </section>

        <section id="services">
            <h2>Services</h2>
            <div class="service-category">
                <div class="service-title">Program Committee</div>
                <p>Area Chair: NeurIPS 2025, ACL 2025, EMNLP 2025</p>
                <p>Reviewer: NeurIPS 2022-2024, ICML 2023-2025, ICLR 2024-2025, CVPR 2025, ICCV 2025, ACL 2020-2024, EMNLP 2020-2024, NAACL 2021-2024, COLM 2024, TPAMI 2023, Scientific Reports 2023</p>
            </div>

            <br>
            <div class="service-category">
                <div class="service-title">Workshop Organizer</div>
                <p><a href="https://dcvlr-neurips.github.io/">DCVLR: Data Curation for Vision Language Reasoning @ NeurIPS 2025</a>, <a href="https://dataworldicml2025.github.io/">DataWorld: Unifying Data Curation Frameworks Across Domains @ ICML 2025</a>, <a href="https://xai4cv.github.io/workshop_cvpr25">The 4th Explainable AI for Computer Vision (XAI4CV) Workshop @ CVPR 2025</a>, <a href="https://sites.google.com/view/mmfm-biomed-2025/">Multimodal Foundation Models for Biomedicine: Challenges and Opportunities @ CVPR 2025</a>, <a href="https://xllms.github.io/">The 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM) @ ACL 2025</a></p>
            </div>

            <br>
            <div class="service-category">
                <div class="service-title">Teaching Assistant</div>
                <p><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/">CS 224N: Natural Language Processing with Deep Learning</a>, <a href="https://biods220.stanford.edu">CS 271: Artificial Intelligence in Healthcare</a></p>
            </div>

            <br>
            <div class="service-category">
                <div class="service-title">Outreach</div>
                <p><a href="https://2025.aclweb.org/calls/student_research_workshop/">Student Research Workshop Mentor @ ACL 2025</a>, <a href="https://www.cs.stanford.edu/admissions-student-resources">Student-Applicant Support Program @ Stanford 2021-2024, <a href="https://cvpr2022.thecvf.com/outreach-activities">Highschool Outreach Program @ CVPR 2022</a></p>
            </div>

            <br>
            <div class="service-category">
                <div class="service-title">Mentoring</div>
                <p>I'm very fortunate to have worked with the following talented undergrads for ≥6 months:</p>
                <p><a href="https://www.linkedin.com/in/elaine-sui/">Elaine Sui (Class of 2024)</a>: Stanford CS MS → Stanford CS PhD <em>w/ SoE Fellowship</em></p>
                <p><a href="https://suyccc.github.io/">Yuchang Su (Class of 2025)</a>: Tsinghua CS Undergrad → Harvard AI in Medicine PhD <em>w/ Fellowship</em></p>
                <p><a href="https://scholar.google.com/citations?user=oPWoXicAAAAJ&hl=en">Rui Li (Class of 2025)</a>: USTC CS Undergrad <em>w/ Highest Honor</em> → Stanford CS PhD</p>
                <p><a href="https://sahithiankireddy14.github.io/">Sahithi Ankireddy (Class of 2025)</a>: Caltech CS Undergrad → Stanford CS MS <em>w/ NSF GRFP Fellowship</em></p>
                <p><a href="https://leo1oel.github.io/">Yiming Liu (Class of 2027)</a>: Tsinghua CS Undergrad</p>
            </div>
        </section>

        <section id="miscellaneous">
            <h2>Miscellaneous</h2>
            <div class="misc-item">
                <p>I enjoy reading books. Some of my favorites: <em>To Live (Hua Yu)</em>, <em>Walden (Henry David Thoreau)</em>, <em>Principles of Economics (N. Gregory Mankiw)</em>. I enjoy hiking, jogging, and swimming. I am a fan of classical music, and I was fortunate to learn basics about how to play the guitar, piano, and pipa at Tsinghua University.</p>
            </div>
        </section>
    </main>

    <footer>
        <p><a href="https://github.com/yuhui-zh15/Minimal-Academic-Website/">© 2025 Yuhui Zhang</a>. Last updated: February 2025. Visit count: <img src="https://hitwebcounter.com/counter/counter.php?page=12074248&style=0007&nbdigits=5&type=page&initCount=16883" title="Counter Widget" Alt="Visit counter For Websites" border="0" /></p>
    </footer>

    <!-- Modal for displaying original images -->
    <div id="imageModal" class="modal">
        <span class="modal-close" onclick="closeModal()">&times;</span>
        <img class="modal-content" id="modalImage">
    </div>

    <!-- Schema.org structured data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Yuhui Zhang",
      "url": "https://cs.stanford.edu/~yuhuiz/",
      "image": "images/yuhui.jpeg",
      "sameAs": [
        "https://scholar.google.com/citations?user=X-Agfu8AAAAJ",
        "https://twitter.com/Zhang_Yu_hui",
        "https://github.com/yuhui-zh15",
        "https://www.linkedin.com/in/yuhui-zhang-223325121/"
      ],
      "jobTitle": "Postdoctoral Scholar",
      "worksFor": {
        "@type": "Organization",
        "name": "Stanford University"
      },
      "alumniOf": {
        "@type": "Organization",
        "name": "Stanford University, Tsinghua University"
      },
      "description": "Researcher in multi-modal machine learning and applications to biomedicine"
    }
    </script>

    <script src="scripts.js?v=20251026"></script>
</body>
</html>
